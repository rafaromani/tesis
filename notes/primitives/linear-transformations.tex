\section*{Linear transformations}

We define two ways of applying linear transformations to an specific vector. Both have the effect of corrupting every vector except the one with the result. The first one saves the answer in a different vector to the right of the one receiving the transformation. Note that this requires adding a extra character at the end (so we get and extra vector) if we want to apply a transformation to the last vector, which we will need for the normalizations. This is uncomfortable for doing CoT. Thats why we also introduce the second way of applying linear transformation which saves the answer in the same vector, but requires to double the embedding size.

\subsection*{Linear transformation storing to the right}

In this section we will see how to apply a linear transformation to one of the vectors and save the output in another more to the right, in other words: $h_j^{l+c} = M h_i^l$ for some $i < j$ previously chosen.

Note that after this procedure all the vector will have their values corrupted with the exception of $h_j$. 

\bigskip

First we will make $h_j = 0$ at the coordinates we are interested (this can be achieved with the previous sections). Then with an ATTN layer we will add $M h_i$.

Using the previous sections we can assume that every vector's last two coordinates are $(0,1)$, except $h_i$'s which are $(1,0)$. These numbers would have been set by the encoding and preserved trough the previous layers.

Assuming this we will construct $W_K$ and $W_Q$ such that:

\[
\langle q_{j}, k_{i'} \rangle = \begin{cases}
    -\infty &\text{ if } i' \neq i \\
    0  &\text{ if } i' = i 
\end{cases}
\]

Note that the $W_Q$ and $W_K$ that we are looking for are the following:

\begin{align*}
    & W_Q = \left(\begin{matrix}
    &0      &\hdots &0      &0          \\
    &\vdots &\ddots &\vdots &\vdots     \\
    &0      &\hdots &0      &0          \\
    &0      &\hdots &0      &-\infty    \\
\end{matrix}\right)
    &W_K = \left(\begin{matrix}
    &0      &\hdots &0      &0      &0      \\
    &\vdots &\ddots &\vdots &\vdots &\vdots \\
    &0      &\hdots &0      &0      &0      \\
    &0      &\hdots &0      &0      &1      \\
\end{matrix}\right)
\end{align*}


since


\begin{align*}
    &q_{j} = W_Q \; h_j = \left(\begin{matrix}
        0 \\
        \vdots \\
        0 \\
        -\infty
    \end{matrix}\right)
    &k_{i'} = W_K \; h_{i'} = \left(\begin{matrix}
        0 \\
        \vdots \\
        0 \\
        (h_{i'})_{d} \\
    \end{matrix}\right)
\end{align*}

so $\langle q_{j}, k_{i'} \rangle = -\infty *(h_{i'})_{d}$

\bigskip

Taking $W_O = M$ y $W_V = id$ we get what we were looking for. 

This works because $e^{-\infty} = 0$ and $softmax(-\infty, \dots, -\infty, 0) = (0, \dots, 0, 1)$, which makes $(s_j)_{i'} = \mathbb{I}(i' = i)$. In summary:

\[
  W_O \sum_{i'=0}^{n} (s_j)_{i'} v_{i'} = 
  W_O \sum_{i'=0}^{j} (s_j)_{i'} v_{i'} = 
  W_O \sum_{i'=0}^{j} (s_j)_{i'} h_{i'} = \]\[ =
  W_O \sum_{i'=0}^{j} \mathbb{I}(i' = i) h_{i'} = 
  W_O \; h_i
\]


\subsection*{Linear transformation doubling the embedding size}

As it was said before, if we want to store the result of the linear transformation in the same vector (because of the error given by the finite representation)\footnote{maybe expand this?} we will have to double the embedding size.

The linear transformation will be realized trough two layers of ATTN. The first layer will store the answer of the linear transformation in the second half of the embedding and overwrite the first one with zeros.
Then, the second layer will swap the two parts so the answer its finally stored in the first part of the embedding. It will also overwrite the second half with zeros so it can be reused.

First, its needed to extend all previous matrices and embeddings from dimension $d$ to dimension $2*d + 1$. This has to be done with zeros in everyplace with the exception of the positional embedding and the last coordinate of the vector we want to multiply. The plus $1$ came from the flag and the times $2$ for having space to save the result.


As we assumed before, the vector we will want to multiply by a matrix will be flagged with a $1$ in the last coordinate and all the other will have a $0$.
As seen before the matrices for $W_Q$ and $W_K$ will be such that $h_i$ only attends to its self:

\begin{align*}
    & W_Q = W_K = \left(\begin{matrix}
    &0      &\hdots &0      &0          \\
    &\vdots &\ddots &\vdots &\vdots     \\
    &0      &\hdots &0      &0          \\
    &0      &\hdots &0      &\infty    \\
\end{matrix}\right)
\end{align*}

Then,

\begin{align*}
    & (W_Q h_{i'})_j = (W_K h_{i'})_j = \begin{cases}
    0       &\text{ if } j \neq 2*d+1               \\
    0       &\text{ if } j = 2*d+1 \land i' \neq i  \\
    \infty  &\text{ if } j = 2*d+1 \land i' = i
\end{cases}
\end{align*}

$W_V$ will be the identity and $W_O$ will be:
\begin{align*}
    & W_O = \left(\begin{matrix}
    &-id^{\mathbb{R}^{d\times d}}     &0^{\mathbb{R}^{d\times d}}     &0^{\mathbb{R}^d} \\
    &M                                &0^{\mathbb{R}^{d\times d}}     &0^{\mathbb{R}^d} \\
    &0^{\mathbb{R}^{1\times d}}       &0^{\mathbb{R}^{1\times d}}     &0^{\mathbb{R}}
\end{matrix}\right)
\end{align*}
Where $M$ is the matrix of the linear transformation wanted to apply.


Note than in the ATTN vector, all the values will be $0$ except for $\langle q_i, k_i\rangle$ which will be $1$. Lets call $h_i^1$ to the first half of $h_i$. Then, the output of the ATTN layer of the $i$th vector will be: $(-h^1_i, Mh^1_i, 0)$ which added to $h_i$ gives us $(0^d, Mh^1_i, 1)$.

In the second layer we only need to swap the halves of $h_i$. For that we will use the same query key and value matrices but the $W_O$ will be:

\begin{align*}
    & W_O = \left(\begin{matrix}
    &0                                &id^{\mathbb{R}^{d\times d}}     &0^{\mathbb{R}^d} \\
    &-id^{\mathbb{R}^{d\times d}}      &0^{\mathbb{R}^{d\times d}}     &0^{\mathbb{R}^d} \\
    &0^{\mathbb{R}^{1\times d}}       &0^{\mathbb{R}^{1\times d}}     &0^{\mathbb{R}}
\end{matrix}\right)
\end{align*}

Then the output in the $i$th vector of this layer of ATTN will be $(M h_i, -M h_i, 0)$ so after adding it with the previous one we get $(M h_i, 0^{d+1})$. Exactly what we were looking for.
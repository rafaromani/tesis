\section*{Linear transformations}

We define two ways of applying linear transformations to an specific vector. Both have the effect of corrupting every vector except the one with the result. The first one saves the answer in a different vector to the right of the one receiving the transformation. Note that this requieres adding a extra character at the end (so we get and extra vector) if we want to apply a transformation to the last vector, which we will need for the normalizations. This is uncomfortable for doing CoT. Thats why we also introduce a second way of applying linear transformation which saves the answer in the same vector, but requieres to double the embedding size.

\subsection*{Linear transformation storing to the right}

In this section we will see how to apply a linar transformation to one of the vectors and save the output in another more to the right, in other words: $h_j^{l+c} = M h_i^l$ for some $i < j$ previously chosen.

Note that after this procedure all the vector will have their values corrupted with the exception of $h_j$. 

\bigskip

First we will make $h_j = 0$ at the coordinates we are interested (this can be achieved with the previous sections). Then with an ATTN layer we will add $M h_i$.

Using the previous sections we can assume that every vector's last two coordinates are $(0,1)$, except $h_i$'s which are $(1,0)$. These numbers would have been set by the encoding and preserved trough the previous layers.

Assuming this we will construct $W_K$ and $W_Q$ such that:

\[
\langle q_{j}, k_{i'} \rangle = \begin{cases}
    -\infty &\text{ if } i' \neq i \\
    0  &\text{ if } i' = i 
\end{cases}
\]

Note that the $W_Q$ and $W_K$ that we are looking for are the following:

\begin{align*}
    & W_Q = \left(\begin{matrix}
    &0      &\hdots &0      &0          \\
    &\vdots &\ddots &\vdots &\vdots     \\
    &0      &\hdots &0      &0          \\
    &0      &\hdots &0      &-\infty    \\
\end{matrix}\right)
    &W_K = \left(\begin{matrix}
    &0      &\hdots &0      &0      &0      \\
    &\vdots &\ddots &\vdots &\vdots &\vdots \\
    &0      &\hdots &0      &0      &0      \\
    &0      &\hdots &0      &0      &1      \\
\end{matrix}\right)
\end{align*}


since


\begin{align*}
    &q_{j} = W_Q \; h_j = \left(\begin{matrix}
        0 \\
        \vdots \\
        0 \\
        -\infty
    \end{matrix}\right)
    &k_{i'} = W_K \; h_{i'} = \left(\begin{matrix}
        0 \\
        \vdots \\
        0 \\
        (h_{i'})_{d} \\
    \end{matrix}\right)
\end{align*}

so $\langle q_{j}, k_{i'} \rangle = -\infty *(h_{i'})_{d}$

\bigskip

Taking $W_O = M$ y $W_V = id$ we get what we were looking for. 

This works because $e^{-\infty} = 0$ and $softmax(-\infty, \dots, -\infty, 0) = (0, \dots, 0, 1)$, which makes $(s_j)_{i'} = \mathbb{I}(i' = i)$. In summary:

\[
  W_O \sum_{i'=0}^{n} (s_j)_{i'} v_{i'} = 
  W_O \sum_{i'=0}^{j} (s_j)_{i'} v_{i'} = 
  W_O \sum_{i'=0}^{j} (s_j)_{i'} h_{i'} = 
  W_O \sum_{i'=0}^{j} \mathbb{I}(i' = i) h_{i'} = 
  W_O \; h_i
\]


\subsection*{Linear transformation doubling the embedding size}


\section*{$\infty$ Normalization}

\subsection*{Definition}

We will say that a transformer $T_N$ is the $\infty$-normalization of a transformer $T$ if it accepts exactly the same words and the $h_n^L$ of $T_N$ respects:


\begin{align*}
    & (h_n^L)_i =
    \begin{cases}
    x & \text{if } i = 1 \\
    -x & \text{if } i = 2 \\
    0 & \text{cc }
    \end{cases} 
    & \text{ where }
    & x = 
    \begin{cases}
    -\infty & \text{ if $T$ rejects }  \\
    \infty & \text{ if $T$ accepts } \\
    \end{cases} 
\end{align*}


\subsection*{Implementation}
\todo{an error was introduced during the translation. The relu part is missing}
The $OUTPUT$ matrix of the $\infty-$normalized transformer will be the same as the 0-1 normalized. Lets analyze now how to get $h_n^L$ to be what we want.

Without loss of generality we can $\infty$-normalize a 0-1 normalized transformer, so we can assume that $h_n^L = (T_{NO}, T_{YES}, 0, \dots, 0)$

Lets call $x' = T_{YES} - T_{NO}$. If $T$ answers yes, then $x' > 0$ because $T_{YES} > T_{NO}$. If $T$ answers no, then the oposite happens, $x' < 0$ since $T_{NO} > T_{YES}$.

Applying the following linear transformation we get a vector with $x'$ and $-x'$:

\begin{equation*}
    \left(\begin{matrix}
        &-1     &1      &0      &\dots  &0      \\
        &1      &-1     &0      &\dots  &0      \\
        &0      &0      &0      &\dots  &0      \\
        &\vdots &\vdots &\vdots &\ddots &\vdots \\
        &0      &0      &0      &\dots  &0      \\
    \end{matrix}\right) 
    \left(\begin{matrix}
        T_{NO} \\
        T_{YES} \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right) = 
    \left(\begin{matrix}
        T_{YES} - T_{NO} \\
        T_{NO} - T_{YES} \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right)=
    \left(\begin{matrix}
        x' \\
        -x' \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right)
\end{equation*}



Because of the finite representation, it happens that for every $z \ge 0$:

\[z *\infty = \begin{cases}
    0   &\text{ if } z = 0 \\
    y   &\text{ for some } y \ge 1 \text{, if } z \ge 1 \text{ since } z \ge \frac{1}{\infty} \\
\end{cases}\]

Then

\[(z *\infty) *\infty= \begin{cases}
    0       &\text{ if } z = 0 \\
    \infty  &\text{ if } z \ge 1 \\
\end{cases}\]

In the same way, for all $z \le 0$ it holds that:

\[(z *\infty) *\infty= \begin{cases}
    0           &\text{ if } z = 0 \\
    -\infty     &\text{ if } z \ge 1 \\
\end{cases}\]


Therefore, applying twice the linear transformation that multiplies by $\infty$ gives us what we want.

Same as with the 0-1 normalization, using the projection of the first two coordinates as output matrix of $T_N$ preserves the behavior.


\begin{itemize}
    \item If $T$ answered no then the first coordinate will be $\infty$ and the second one $-\infty$ before applying softmax. But for the properties of the finite representation, $e^{-\infty} = 0$ and $e^{\infty} = \infty$. Then the output of the softmax will be $(1,0)$. So the argmax will pick no.
    \item In the same way if $T$ answered yes, the output of the softmax will be $(0,1)$. So the argmax  will pick yes.
\end{itemize}


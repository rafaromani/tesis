\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{Normalización de transformers}
\author{GLyC}
\date{\today}

\begin{document}

\maketitle

Vamos a definir dos nociones de normalización de los transformers. Ambas van a hablar del valor del último vector antes de salir del ciclo. Lo que en el paper llamarían $h_n^L$. Además la matriz de output será \~ la identidad.

\section*{Normalización 0-1}

\subsection*{Definición}

Vamos a decir que un transformer $T_N$ es la 0-1 normalización de un transformer $T$ si el $h_n^L$ de $T_N$ cumple que:

\begin{equation*}
(h_n^L)_i =
\begin{cases}
T_{NO} & \text{si } i = 1 \\
T_{SI} & \text{si } i = 2 \\
0 & \text{cc }
\end{cases}
\end{equation*}

donde $T_{NO}$ y $T_{SI}$ corresponden a los valores que calcula la función $OUTPUT$ antes de hacer el softmax, es decir 

\begin{align*}
    T_{NO} &:= (\Theta_{OUTPUT}(h_n^L))_1 \\
    T_{SI} &:= (\Theta_{OUTPUT}(h_n^L))_2 
\end{align*}


\subsection*{Implementación}

Vamos a tomar un transformer $T$ y obtener su normalizado 0-1. Para ello lo que vamos a hacer es agregar algunas capas más al final de $T$ para conseguir aplicarle una transformación lineal a $h_n^L$.

Dado que $\Theta_{OUTPUT} \in \mathbb{R}^{2\times d}$ basta con extender $\Theta_{OUTPUT}$ con $0$ hasta tener $\Theta_{OUTPUT}' \in \mathbb{R}^{d\times d}$


\begin{equation*}
    \Theta_{OUTPUT}h_n^L =   
    \left(\begin{matrix}
        T_{NO} \\
        T_{SI}
    \end{matrix}\right) \longrightarrow
    \left(\begin{matrix}
        &\Theta_{OUTPUT} &0 &\dots  &0 \\
        &0                &0 &\dots  &0 \\
        &\vdots           &0 &\ddots &\vdots \\
        &0                &0 &\dots  &0 \\
    \end{matrix}\right)h_n^L = 
    \left(\begin{matrix}
        T_{NO} \\
        T_{SI} \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right)
\end{equation*}


Luego la matriz $\Theta_{OUTPUT}$ de este nuevo transformer sería:

\begin{equation*}
    \left(\begin{matrix}
        &1 &0 &0 &\dots &0 \\
        &0 &1 &0 &\dots &0 
    \end{matrix}\right)
\end{equation*}

Dado que 

\begin{align*}
    \left(\begin{matrix}
        &1 &0 &0 &\dots &0 \\
        &0 &1 &0 &\dots &0 
    \end{matrix}\right)
    &\left(\begin{matrix}
        &\Theta_{OUTPUT}h &0 &\dots  &0 \\
        &0                &0 &\dots  &0 \\
        &\vdots           &0 &\ddots &\vdots \\
        &0                &0 &\dots  &0 \\
    \end{matrix}\right)h = \\
    = \left(\begin{matrix}
        &1 &0 &0 &\dots &0 \\
        &0 &1 &0 &\dots &0 
    \end{matrix}\right)
    &\left(\begin{matrix}
        T_{NO} \\
        T_{SI} \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right) = 
    \left(\begin{matrix}
        T_{NO} \\
        T_{SI} \\ 
    \end{matrix}\right)
\end{align*}

Por lo tanto preserva el comportamiento.

Falta ver es cómo aplicarle una transformación lineal a un vector y guardarla al final. Más sobre eso en Requisitos.

\section*{Normalización $\infty$}


\section*{Requisitos}
\todo{mostrar que podemos hacer transformaciones lineales}


\end{document}
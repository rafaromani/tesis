\section*{Requirements}

\subsection*{Embedding with flags}

\todo{flagear vectores con el positional embedding}

\subsection*{Make 0 a vector }

\todo{0}

\subsection*{ATTN y FF controlling which coordinates are affected}
%maybe i should write the equations? is it worth?
Later we will see that we want to go trough layers of attention and feed forward ignoring some coordinates. That means without taking them into consideration and without affecting them in the output.

For example, if we want to ignore the $i$th coordinate trough a layer of attention, it's enough to put zeros in the $i$th column and row of $W_Q, W_K, W_V, W_O$. This can be done with as many coordinates as desired. Note that because the $i$th row of $W_O$ is zero, the output vector of the ATTN layer will have a zero in the $i$th coordinate. This is the point that makes it possible to not just ignore the value of the $i$th coordinate but to not affect it. This happens because the result of the attention is added to the original vector.

The same applies in the feed forward layer, putting zeros in the $i$th column and row of the matrix and vectors of the FF makes it ignore the value of the $i$th coordinate. Also in the result of the FF will appear a zero in the $i$th coordinate.

\subsection*{Linear transformations}

In this section we will see how to apply a linar transformation to one of the vectors and save the output in another more to the right, in other words: $h_j^{l+c} = M h_i^l$ for some $i < j$ previously chosen.

Note that after this procedure all the vector will have their values corrupted with the exception of $h_j$.

\bigskip

First we will make $h_j = 0$ at the coordinates we are interested. Then with an ATTN layer we will add $M h_i$.

Thanks to the previous sections we can assume that $h_i$ is the only vector which has $(1,0)$ in the lasts coordinates and that the rests end in $(0,1)$. These numbers would have been set by the encoding and preserved trough the previous layers.

Assuming this will allow us to construct $W_K$ and $W_Q$ such that:

\[
\langle q_{j}, k_{i'} \rangle = \begin{cases}
    -\infty &\text{ if } i' \neq i \\
    0  &\text{ if } i' = i 
\end{cases}
\]

Note that the $W_Q$ and $W_K$ that we are looking for are the following:

\begin{align*}
    & W_Q = \left(\begin{matrix}
    &0      &\hdots &0      &0          \\
    &\vdots &\ddots &\vdots &\vdots     \\
    &0      &\hdots &0      &0          \\
    &0      &\hdots &0      &-\infty    \\
\end{matrix}\right)
    &W_K = \left(\begin{matrix}
    &0      &\hdots &0      &0      &0      \\
    &\vdots &\ddots &\vdots &\vdots &\vdots \\
    &0      &\hdots &0      &0      &0      \\
    &0      &\hdots &0      &0      &1      \\
\end{matrix}\right)
\end{align*}


since


\begin{align*}
    &q_{j} = W_Q \; h_j = \left(\begin{matrix}
        0 \\
        \vdots \\
        0 \\
        -\infty
    \end{matrix}\right)
    &k_{i'} = W_K \; h_{i'} = \left(\begin{matrix}
        0 \\
        \vdots \\
        0 \\
        (h_{i'})_{d} \\
    \end{matrix}\right)
\end{align*}

so $\langle q_{j}, k_{i'} \rangle = -\infty *(h_{i'})_{d}$

\bigskip

Taking $W_O = M$ y $W_V = id$ we get what we were looking for. This happens because the fact that $e^{-\infty} = 0$ makes that $softmax(-\infty, \dots, -\infty, 0) = (0, \dots, 0, 1)$, which makes $(s_j)_{i'} = \mathbb{I}(i' = i)$ so finally:

\[
  W_O \sum_{i'=0}^{n} (s_j)_{i'} v_{i'} = 
  W_O \sum_{i'=0}^{j} (s_j)_{i'} v_{i'} = 
  W_O \sum_{i'=0}^{j} (s_j)_{i'} h_{i'} = 
  W_O \sum_{i'=0}^{j} \mathbb{I}(i' = i) h_{i'} = 
  W_O \; h_i
\]
















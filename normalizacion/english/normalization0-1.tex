\section*{0-1 Normalization}

\subsection*{Definition}

We will say that a transformer $T_N$ is the 0-1 normalization of a transformer $T$ if the $h_n^L$ of $T_N$ respects:

\begin{equation*}
(h_{n}^L)_i =
\begin{cases}
T_{NO} & \text{if } i = 1 \\
T_{YES} & \text{if } i = 2 \\
0 & \text{cc }
\end{cases}
\end{equation*}

where $T_{NO}$ and $T_{YES}$ correspond to the values of the $OUTPUT$ function before the softmax:

\begin{align*}
    T_{NO} &:= (\Theta_{OUTPUT}(h_n^L))_1 \\
    T_{YES} &:= (\Theta_{OUTPUT}(h_n^L))_2 
\end{align*}


\subsection*{Implementation}

Let $T$ be a transformer, we will construct its 0-1 normalized. The idea is to add some layers at the end of $T$ such that they multiply $h_n^L$ by $\Theta_{OUTPUT}$. Since the dimensions doesn't match, it's needed to extend the matrix with zeros obtaining $\Theta_{OUTPUT}' \in \mathbb{R}^{d\times d}$.


\begin{equation*}
    \Theta_{OUTPUT}h_n^L =   
    \left(\begin{matrix}
        T_{NO} \\
        T_{YES}
    \end{matrix}\right) \longrightarrow
    \left(\begin{matrix}
        &\Theta_{OUTPUT} &0 &\dots  &0 \\
        &0                &0 &\dots  &0 \\
        &\vdots           &0 &\ddots &\vdots \\
        &0                &0 &\dots  &0 \\
    \end{matrix}\right)h_n^L = 
    \left(\begin{matrix}
        T_{NO} \\
        T_{YES} \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right)
\end{equation*}


If we are capable to apply the OUTPUT matrix of $T$ inside the layers of the transformer, then the matrix $\Theta_{OUTPUT}$ of $T_N$ will be just a projection of the firsts two coordinates:


\begin{equation*}
    \left(\begin{matrix}
        &1 &0 &0 &\dots &0 \\
        &0 &1 &0 &\dots &0 
    \end{matrix}\right)
\end{equation*}

Since

\begin{align*}
    \left(\begin{matrix}
        &1 &0 &0 &\dots &0 \\
        &0 &1 &0 &\dots &0 
    \end{matrix}\right)
    &\left(\begin{matrix}
        &\Theta_{OUTPUT}h &0 &\dots  &0 \\
        &0                &0 &\dots  &0 \\
        &\vdots           &0 &\ddots &\vdots \\
        &0                &0 &\dots  &0 \\
    \end{matrix}\right)h_n^L = \\
    = \left(\begin{matrix}
        &1 &0 &0 &\dots &0 \\
        &0 &1 &0 &\dots &0 
    \end{matrix}\right)
    &\left(\begin{matrix}
        T_{NO} \\
        T_{YES} \\ 
        0 \\
        \vdots \\
        0
    \end{matrix}\right) = 
    \left(\begin{matrix}
        T_{NO} \\
        T_{YES} \\ 
    \end{matrix}\right)
\end{align*}

It's easy to see that the normalized transformer preserves the behavior.
